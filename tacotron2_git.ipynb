{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tacotron2_git.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCuomo/FakeVoice/blob/main/tacotron2_git.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-5rnrMVt8bS"
      },
      "source": [
        "# Sample Colab notebook for installation / training / synthesis of a Tacotron-2 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4luFjhOiZrox"
      },
      "source": [
        "Welcome to my attempt at a comprehensive notebook for training custom tacotron-2 models for audio deepfakes. This is a work in progress, and will recieve periodic updates. I'm an amateur when it comes to all of this, and will gladly accept advice, clarification, useful edits, and optimizations.\n",
        "\n",
        "**This particular notebook is set up to train from the NVIDIA's published pretrained tacotron2 and waveglow models. The WAV files in your dataset should be Mono, 22050 hz, with 16-bit Microsoft PCM encoding.Each WAV file should be between 2 and 15 seconds. Between 2 and 10 seconds is ideal.**\n",
        "\n",
        "If you're new to Colab, below are some notes and a couple of the techniques I use to run things:\n",
        "\n",
        "You can think of a colab notebook as a virtual machine running on a Google server in some far-off datacenter. As a result, anything that is installed/stored on the VM is deleted whenever the session ends. To get around this limitation, you will mount your google drive to the VM instance, and read/write to that directory.\n",
        "\n",
        "<!> This is your standard bash/command line execution thing. Pop it before a command to execute via the command line instead of as a script.\n",
        "\n",
        "<%> This is a magics call. Some commands, like cd, won't execute with !. This lets you do that, and is especially useful when condensing code that would normally span multiple cells into a single cell. You might see me cd into a folder without this in some cells--that's because if a command like cd is in a cell alone, \"automagics\" inserts the % for you. But this doesn't work if it shares a cell with other operations.\n",
        "\n",
        "**A last important note: make sure you disconnect the runtime when you're done, so google doesn't clock you as using a GPU when you're actually not. Failure to do so may get you downgraded to a worse GPU temporarily.** Go to the \"connect\" dropdown in the top right corner, select \"manage sessions,\" and then \"terminate\" to end the session and disconnect the runtime.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNZ4yeD_cOR5"
      },
      "source": [
        "# Environment Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F73iR5jJuReE"
      },
      "source": [
        "Make sure you're using a GPU runtime -- go to \"runtime\" at the top of the page > \"change runtime type\" > select \"GPU\" under Hardware Accelerator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOxq6D50wljB"
      },
      "source": [
        "Run this cell to check what kind of GPU you're connected to - Since colab is free, you don't get to choose. Anything not P100 or V100 can cause problems; see the bottom of the notebook for troubleshooting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2w_9VI-lh2Y"
      },
      "source": [
        "%tensorflow_version 1.x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zLQvHbzSP7t"
      },
      "source": [
        "!nvidia-smi -L\n",
        "#P100 = Good\n",
        "#V100 = Amazing\n",
        "#T4 = Crap\n",
        "#P4 = Crap\n",
        "#K80 = Slow\n",
        "#This nugget of info copied from the MLP community's tts project/cookiePPP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCTRI8k6-R5v"
      },
      "source": [
        "Install EPITRAN for IPA text preprocessing. This step is strictly optional, but transliteration from English graphemes to language-agnostic phonemes will almost may improve model generalization and pronunciation. ARPABET will likely be better still, and is enabled by default in hparams.py via `preprocessing = \"arpabet\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj_Z2XfC-4rw"
      },
      "source": [
        "# You have to do this in root for some reason. If you install into a directory mounted via drive, you get bad interpreter errors.\n",
        "!pip install epitran\n",
        "!git clone https://github.com/festvox/flite.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66sAy-7qk-BY"
      },
      "source": [
        "!pip install epitran\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgWqtruPMJtG"
      },
      "source": [
        "%cd '/content/flite/'\n",
        "!./configure && make\n",
        "!sudo make install\n",
        "%cd '/content/flite/testsuite/'\n",
        "!make lex_lookup\n",
        "!sudo cp lex_lookup /usr/local/bin\n",
        "%cd '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfApCHYiw9hk"
      },
      "source": [
        "Mount your google drive. If you haven't mounted a drive via colab before, it just lets you cd in and out of your storage on google drive, and lets you write and read files from this notebook. The directory you see when you go to the normal google drive interface is typically \"/content/drive/My Drive\". The below cell will create the directory \"ML\" in your \"My Drive\" folder if does not already exist, and then drop you into the folder. After mounting, you can access files from the left sidebar. Double click on a file to edit it in a text editor.\n",
        "\n",
        "NOTE: It's best not to upload large files via the Google Colab sidebar; it doesn't like uploads of more than 10MB or so. It's typically best to upload "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_6Rey1AbmIa"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "\n",
        "# %cd '/content/drive/'\n",
        "\n",
        "# import os\n",
        "# if not os.path.exists('My Drive/ML/'):\n",
        "#     os.makedirs('My Drive/ML/')\n",
        "# else:\n",
        "#     print(\"\\nDirectory \" + '\"/My Drive/ML\"' + \" already exists, skipping creation and navigating to directory.\\n\")\n",
        "\n",
        "# %cd '/content/drive/My Drive/ML/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmghhwblxUTQ"
      },
      "source": [
        "Clone the Audio DeepFakes TacoTron2 repo. Because we're cloning into a subdirectory of your google drive, you should only have to clone the repo the first time you run the notebook. If you need to update your repo, you can run the \"!git fetch --all\" cell. Note that this will overwrite any changes you've made in your hparams.py file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjkOISnjPF-d"
      },
      "source": [
        "!git clone https://github.com/scripples/tacotron2.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iolwPVcF0qy"
      },
      "source": [
        "Run the following cell if you need to update to the latest version of the repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUccyaZ9VnP-"
      },
      "source": [
        "!git fetch --all\n",
        "!git reset --hard origin/master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RBW6_A_BWEZ"
      },
      "source": [
        "Install the required modules via requirements.txt. You will need to reinstall requirements.txt every time you restart the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGhX4nmRBVqo"
      },
      "source": [
        "!apt-get install sox\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BDjB35QDon6"
      },
      "source": [
        "After installing it will ask you to restart the runtime. Do so. If you're unable to reconnect, check the troubleshooting segment at the bottom of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5kA-Atc42HO"
      },
      "source": [
        "After retarting the runtime, cd back into the tacotron2 directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNkmbScBXw2d"
      },
      "source": [
        "cd \"tacotron2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgiyZE8Oci56"
      },
      "source": [
        "Install the waveglow submodule. Like cloning the repo, you should only need to do this on the first run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3E0V1IvuPNJ"
      },
      "source": [
        "!git submodule init\n",
        "!git submodule update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFLnPdEEcYXZ"
      },
      "source": [
        "The following cell will download the pretrained tacotron2 and waveglow models to their appropriate folders. Ignore the tacotron2 folder inside of the waveglow folder. I don't know why they did that, but that particular folder is unused. Again, you will only need to do this on first run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiEb1znuYAhU"
      },
      "source": [
        "!gdown --id 1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA\n",
        "%cd \"tacotron2/waveglow\"\n",
        "!gdown --id 1rpK8CzAAirq9sWZhe9nlfvxMF1dRgFbF\n",
        "%cd \"tacotron2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xHPMZhl5lkM"
      },
      "source": [
        "Congratulations! You've just installed Tacotron-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRhSfKCI5vov"
      },
      "source": [
        "# Upload your dataset via Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vyVF5gWxsS5"
      },
      "source": [
        "This is where you upload your dataset and the filelists that train.py needs to run. Upload your directory containing the WAV files to the location of your choice. You will need to preprocess your train.txt and val.txt so that the filename also points to the directory of your uploaded files. Also make sure to upload your files through Google Drive, and not through Colab. Colab doesn't like it when you upload 1000+ wav files through its sidebar.\n",
        "\n",
        "You need three things: A directory containing the WAV files, and two files which contain wav|transcription indexes. You can find an example in filelists/ljs_audio_text_val_filelist.txt. In this case, the wavs are found in the DUMMY directory, and the wavs and transcripts are separated by the \"|\" character.\n",
        "\n",
        "In my case, I have my WAVs in \"drive/My Drive/ML/Datasets/saltzvoice/wavs\". This means that my train.txt and val.txt will look like this:\n",
        "\n",
        "../Datasets/saltzvoice/wavs/saltz_0731.wav|After that, however, the bump disappears and the debt remains.\n",
        "../Datasets/saltzvoice/wavs/saltz_0096.wav|If you’re an aspiring artist, I want you to remember: Nothing happens if you’re not working.\n",
        "\n",
        "So they're pointed up one directory from where train.py is running, and back down into where I'm keeping my wavs.\n",
        "\n",
        "Next, you need to upload your train.txt and val.txt. I keep them in \"/ML/tacotron2/filelists\", or in the same folder as the \"wavs\" folder.\n",
        "\n",
        "Finally, you need to update hparams.py, which is in the tacotron2 directory. Under Data parameters, update training_files and validation_files to point to the directory containing your training and validation .txt files. Then, under Optimization_Hyperparameters, lower batch_size to something like 16. If you run into an out of memory error, you'll have to lower it even more. If you're training on files at 44.1khz or 32-bit PCM, you'll have to work with a much lower batch size. Lower batch size means slower convergence, and I've heard reports that extremely low batch sizes may have difficulty converging at all.\n",
        "\n",
        "I also change the iters_per_checkpoint parameter to something like 500, so I can test the model's output every 500 steps, as opposed to every 2000. 2000 makes sense if you're training a model from scratch, but if you're fine-tuning/transfer learning, it may converge much sooner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi3DSnbvWZ3l"
      },
      "source": [
        "!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwWpSiCgXTTn"
      },
      "source": [
        "!tar xvfj 'LJSpeech-1.1.tar.bz2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p704TS1CgsPl"
      },
      "source": [
        "# Modificaciones\n",
        "Descargue metadata.csv, le cambie la extension a txt y en notepad agregue la carpeta:  \n",
        "\n",
        "```\n",
        "wave_name|transcript   --->  path/wave_name|transcript\n",
        "```\n",
        "\n",
        "\n",
        "Despues lo dividi en test, train, val y los cargue al colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7edyeZJw1lBx"
      },
      "source": [
        "You can use the following cell to preprocess your WAVs for a marginal improvment in training. Depending on the size of your dataset, this may take a minute. Just make sure you're running it from inside the directory containing your WAVs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbtJRGxn5rgf"
      },
      "source": [
        "%cd \"dataset/wavs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cikue-7FuupA"
      },
      "source": [
        "# More code snarfed/adapted from cookie/synthbot\n",
        "\n",
        "# Adds a slight fade to both ends to prevent pops / sharp noises at the beginning. Ends are then padded with silence and then stripped back, ensures all files have same silence padding.\n",
        "\n",
        "%%bash\n",
        "\n",
        "for file in *.wav; do\n",
        "    cp \"$file\" \"tmp.wav\";\n",
        "    sox -q \"tmp.wav\" \"$file\" pad .2 .2 silence 1 0.1 0.1% reverse silence 1 0.1 0.1% reverse;\n",
        "done\n",
        "\n",
        "%rm tmp.wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB62fDAA5PgL"
      },
      "source": [
        "%rm tmp.wav\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAlkvQ2jU-r1"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1VhNgKIh5KF"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9pXMhq50sV0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Time to train your model. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-p_cvc802Ek"
      },
      "source": [
        "This cell will train the model, starting with the pretrained weights from tacotron2_statedict.py. This will make it converge faster. It'll write the newly trained model checkpoints to outdir and the logs for tensorboard analysis to logdir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucLf8sj0lOvV"
      },
      "source": [
        "!pip install g2p-en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYqQYOgwYUzZ"
      },
      "source": [
        "!pip install eng-to-ipa\n",
        "!pip install Unidecode==1.0.22\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obzUyPFCHI9c"
      },
      "source": [
        "%cd \"/content/tacotron2\"\n",
        "!python train.py --output_directory=outdir --log_directory=logdir2 -c tacotron2_statedict.pt --warm_start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7S57hDz22ng"
      },
      "source": [
        "Hopefully your model is training! If you want to see how it's doing, you can use my tensorboard notebook, also in the discord. If it throws an error, let me know in the discord and I'll get back to you as soon as I can.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjOA3LMPdYqm"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Training Waveglow (Optional)\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scQ1aHM_fxqk"
      },
      "source": [
        "Waveglow is the model actually responsible for turning spectrograms into audio data. By default, the pretrained waveglow model will synthesize decent quality speech without additional training, though you may notice some sibilance and grain issues. You can improve the audio quality of the synthesis by training waveglow in addition to tacotron2. Waveglow has been shown to perform well when training from scratch even on small datasets.\n",
        "\n",
        "Training waveglow takes a long time--about a week's worth of training for a decent model (270,000 iterations on an hour-long dataset). This is best thought of as a polishing step--once you're happy with your Tacotron2 model and you want to improve final audio quality, consider training waveglow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR-Pmv_pg8I3"
      },
      "source": [
        "Waveglow doesn't care about your audio transcriptions; all it cares about are files themselves. Create a training and a test list from the directory containing your files. Make sure you're in the waveglow directory first, if you're not already. Note the paths in the shuffle command; you may have to edit them for your preferred dataset storage location. Because my wavs are in \"ML/Datasets/saltzvoice/wavs\", I have to point up a couple of levels, and then back down into my dataset location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMvVYpC5-FxD"
      },
      "source": [
        "Install apex and TensorboardX.\n",
        "\n",
        "This could take a minute. Make a cup of tea or something."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QsRDK3M-HjY"
      },
      "source": [
        "%cd \"/content/drive/My Drive/ML/tacotron2\"\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd \"/content/drive/My Drive/ML/tacotron2/apex\"\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
        "%cd \"/content/drive/My Drive/ML/tacotron2\"\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N1eYSuC1spY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Create train and test files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MllN2L99iJt_"
      },
      "source": [
        "cd /content/drive/My Drive/ML/tacotron2/waveglow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkeqvdAi5pnx"
      },
      "source": [
        "Get them submodules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8nPORkw5mMF"
      },
      "source": [
        "!git submodule init\n",
        "!git submodule update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8_J8tIUr33g"
      },
      "source": [
        "# Read and shuffle file list, strip newlines\n",
        "%ls ../../Datasets/saltzvoice/wavs/*.wav | shuf > shuf.txt\n",
        "file = \"shuf.txt\"\n",
        "with open(file, \"r\", encoding = \"utf-8\") as wkfile:\n",
        "  text = wkfile.readlines()\n",
        "text = [line.replace('\\n', '') for line in text]\n",
        "\n",
        "#Create two sublists by percentage, write to train and text files\n",
        "pct = 0.8    ##0.8 = 80/20 split##\n",
        "train = text[:int(len(text)*pct)]\n",
        "val = text[len(train):]\n",
        "\n",
        "\n",
        "with open(\"train_files.txt\", \"w\", encoding = \"utf-8\") as fobj:\n",
        "    for x in train:\n",
        "        fobj.write(x + \"\\n\")\n",
        "with open(\"test_files.txt\", \"w\", encoding = \"utf-8\") as fobj:\n",
        "    for x in val: \n",
        "        fobj.write(x + \"\\n\")\n",
        "\n",
        "#cleanup\n",
        "%rm shuf.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axJXXTJD10MZ"
      },
      "source": [
        "If training from a pretrained model, change checkpoint_path in config.json to point to your pretrained model.\n",
        "\n",
        "You will also have to comment out the lines:\n",
        "\n",
        "\n",
        "```\n",
        "iteration = checkpoint_dict['iteration']\n",
        "optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "And insert this line jsut below:\n",
        "\n",
        "`iteration = 1`\n",
        "\n",
        "in the waveglow train.py. Eventually I'll make my own fork of all of this and it'll be automatic, but until then this is what you do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDjO6Ojc9AQ7"
      },
      "source": [
        "Train baby train. The loss will be negative, so don't panic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMryze0510fH"
      },
      "source": [
        "#From the NVIDIA waveglow repo, training from scratch: \"100 epochs make your model be able to generate reasonable voice. But for channel size 512 version, you need more than 500 epochs to get high quality voice.\" Note that one epoch doesn't equal one step, one epoch is just one cycle through your entire dataset, so your mileage may vary.\n",
        "%mkdir \"/content/drive/My Drive/ML/tacotron2/waveglow/checkpoints\"\n",
        "!python train.py -c config.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39WnWfc2z5wy"
      },
      "source": [
        "In the event you encounter the error:\n",
        "```\n",
        "File \"/usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py\", line 30, in call\n",
        "    *args)\n",
        "RuntimeError: A tensor was not contiguous.\n",
        "```\n",
        "This means that Apex (an optimizer) is broken. This started happening sometime around July of 2020. To turn it off, open config.json in the waveglow folder and set `\"fp16_run\": false`. Save and restart your runtime, and now it should train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DaMQvIn6cMu",
        "cellView": "form"
      },
      "source": [
        "#@title Some stuff to maybe integrate later, don't run this cell\n",
        "\"\"\"\n",
        "# FOR LATER EXPERIMENTATION\n",
        "# https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/nvidia_deeplearningexamples_waveglow.ipynb#scrollTo=BTMWFuOq6R9W\n",
        "\n",
        "# remove weightnorm to improve quality?\n",
        "\n",
        "# Loading waveglow:\n",
        "\n",
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow = waveglow.to('cuda')\n",
        "waveglow.eval()\n",
        "\n",
        "# Loading tacotron (make local instead of hub)\n",
        "\n",
        "tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2')\n",
        "tacotron2 = tacotron2.to('cuda')\n",
        "tacotron2.eval()\n",
        "\n",
        "# Actually running the thing:\n",
        "\n",
        "# preprocessing\n",
        "sequence = np.array(tacotron2.text_to_sequence(text, ['english_cleaners']))[None, :]\n",
        "sequence = torch.from_numpy(sequence).to(device='cuda', dtype=torch.int64)\n",
        "\n",
        "# run the models\n",
        "with torch.no_grad():\n",
        "    _, mel, _, _ = tacotron2.infer(sequence)\n",
        "    audio = waveglow.infer(mel)\n",
        "audio_numpy = audio[0].data.cpu().numpy()\n",
        "rate = 22050\n",
        "\n",
        "# Save with \n",
        "write(\"audio.wav\", rate, audio_numpy)\n",
        "\n",
        "#play with ipyton:\n",
        "from IPython.display import Audio\n",
        "Audio(audio_numpy, rate=rate)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0MmStOhdZNb"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Synthesize the audio from your model.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYYwDsvnjcG4"
      },
      "source": [
        "cd \"/content/drive/My Drive/ML/tacotron2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hihL-mgy3LX0"
      },
      "source": [
        "Import and intialize some stuff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f6CYwLHQwmI"
      },
      "source": [
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "import sys\n",
        "sys.path.append('waveglow/')\n",
        "import numpy as np\n",
        "import torch\n",
        "import epitran\n",
        "import eng_to_ipa as ipa\n",
        "import re\n",
        "\n",
        "\n",
        "from model import Tacotron2\n",
        "from layers import TacotronSTFT, STFT\n",
        "from audio_processing import griffin_lim\n",
        "from train import load_model\n",
        "from text import text_to_sequence\n",
        "from denoiser import Denoiser\n",
        "from text.cleaners import english_cleaners\n",
        "from utils import make_arpabet\n",
        "from hparams import create_hparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3uqyPCynzpR"
      },
      "source": [
        "def plot_data(data, figsize=(16, 4)):\n",
        "    fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
        "    for i in range(len(data)):\n",
        "        axes[i].imshow(data[i], aspect='auto', origin='bottom', \n",
        "                       interpolation='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNUvBqNun1rQ"
      },
      "source": [
        "hparams = create_hparams()\n",
        "hparams.sampling_rate = 22050"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZpOQQYP3Qz7"
      },
      "source": [
        "Point checkpoint_path to the checkpoint you want to synthesize from. If you've saved a new checkpoint every 500 iterations and you've trained for 8000 steps, you should have 16 to choose from. You can also synthesize from tacotron2_statedict.pt, to test the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv-Qlv4hn3sY"
      },
      "source": [
        "checkpoint_path = \"outdir/checkpoint_40\"\n",
        "model = load_model(hparams)\n",
        "model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n",
        "_ = model.cuda().eval().half()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAq_i6K_3jCM"
      },
      "source": [
        "Point waveglow in the right direction. Change waveglow_path to your finetuned model, if you've done that. But NVIDIA's pretrained model is still pretty good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj0Keemrn7gu"
      },
      "source": [
        "waveglow_path = '/content/tacotron2/waveglow_256channels_universal_v5.pt'\n",
        "waveglow = torch.load(waveglow_path)['model']\n",
        "waveglow.cuda().eval().half()\n",
        "for k in waveglow.convinv:\n",
        "    k.float()\n",
        "denoiser = Denoiser(waveglow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGDPTVt3mJP"
      },
      "source": [
        "Tell it what to say."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsNYyuf-n9o8"
      },
      "source": [
        "text = \"Who lives in a pineapple under the. \"\n",
        "\n",
        "#epitran preprocessing loop\n",
        "hparams = create_hparams()\n",
        "epi = epitran.Epitran('eng-Latn', ligatures = True)\n",
        "if hparams.preprocessing == \"ipa\":\n",
        "  text = ipa.convert(english_cleaners(text))\n",
        "  foreign_words = re.findall(r\"[^ ]{0,}\\*\", text)\n",
        "  for word in foreign_words:\n",
        "    text = text.replace(word, epi.transliterate(word[0:len(word)-1]))\n",
        "if hparams.preprocessing == \"arpabet\":\n",
        "  text = make_arpabet(text)\n",
        "\n",
        "print(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivm4yj3QAks_"
      },
      "source": [
        "\n",
        "#text sequencer\n",
        "if hparams.preprocessing is not None:\n",
        "  sequence = np.array(text_to_sequence(text, None))[None, :]\n",
        "else:\n",
        "  sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
        "sequence = torch.autograd.Variable(\n",
        "    torch.from_numpy(sequence)).cuda().long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXAW3BwNAYJ5"
      },
      "source": [
        "text='{HH IH1 Z} {F EH1 R} {P L EY1} {F AO1 R} {K Y UW1 B AH0} {K AH0 M IH1 T IY0} {AE0 K T IH1 V AH0 T IY0 Z}, {HH AW2 EH1 V ER0}, {M EY1 D} {IH1 T} {M AO1 R} {D IH1 F AH0 K AH0 L T} {F AO1 R} {HH IH1 M} {T UW1} {AH0 B T EY1 N} {AH1 DH ER0} {EH0 M P L OY1 M AH0 N T}.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "una0LxN63ou9"
      },
      "source": [
        "Plot the MELS and synthesize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grsKfTjiwlVG"
      },
      "source": [
        "mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
        "plot_data((mel_outputs.float().data.cpu().numpy()[0],\n",
        "           mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
        "           alignments.float().data.cpu().numpy()[0].T))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "265Pngy7jpc1"
      },
      "source": [
        "Play it back. The first is the raw audio output, and the second is a denoised version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpLJtnomn_Rz"
      },
      "source": [
        "with torch.no_grad():\n",
        "    audio = waveglow.infer(mel_outputs_postnet, sigma=0.666)\n",
        "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6EVPr0XwvOm"
      },
      "source": [
        "audio_denoised = denoiser(audio, strength=0.01)[:, 0]\n",
        "ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmL-Ei_44TWh"
      },
      "source": [
        "#Some troubleshooting\n",
        "\n",
        "---\n",
        "\n",
        "This runs pretty smoothly for me, however I've occasionally had the problem where after installing requirements.txt, I'll be prompted to restart the runtime. This is normal--if it asks you to do so, just re-cd into whatever folder you were in before you restarted it and continue as normal. However, sometimes when restarting the runtime the session will fail to reconnect, and will make you start over. If this happens, check to see what your GPU is. On anything other than P100 and V100, the card may be running out of memory and you will be unable to reconnect. Because of the way Colab works, if you've been training a lot recently it may downgrade you temporarily, and you'll have to wait some time before it upgrades you again. There's no documentation on how this works, so you'll just have to wait and see. If you're really eager, you can subscribe to Colab Pro and it'll give you priority access to better GPUs, but it's still no garauntee that you won't be downgraded from time to time.\n",
        "\n",
        "__During training__\n",
        "\n",
        "Problem: Runtime Error: shape '[1, 1, 1234]' is invalid for input size of [12345]\n",
        "\n",
        "Fix: Make sure your WAV files are mono, and not stereo.\n",
        "\n",
        "Thanks to Sticky and Rutherfox from the discord for helping debug. Thanks to CamJ for fixes to several filepaths and making it generally more user-friendly. Thanks to Samurzl for the IPA preprocessing integration and the code used for processing the synthesized text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NajFmx7NDtrs"
      },
      "source": [
        "MISC\n",
        "\n",
        "On training smaller datasets: https://github.com/NVIDIA/tacotron2/issues/344"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlIY8eIfOV3R"
      },
      "source": [
        "!!SCRIPP'S TESTING ZONE DON'T RUN ME!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi5xfhap36sK"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoHgFxnK32OK"
      },
      "source": [
        " cd \"/content/drive/My Drive/ML/tacotron2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oZWC2Z0Wu_c"
      },
      "source": [
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "import sys\n",
        "sys.path.append('waveglow/')\n",
        "import numpy as np\n",
        "import torch\n",
        "import epitran\n",
        "import eng_to_ipa as ipa\n",
        "import re\n",
        "import os\n",
        "\n",
        "from model import Tacotron2\n",
        "from layers import TacotronSTFT, STFT\n",
        "from audio_processing import griffin_lim\n",
        "from train import load_model\n",
        "from text import text_to_sequence\n",
        "from denoiser import Denoiser\n",
        "from text.cleaners import english_cleaners\n",
        "from utils import make_arpabet\n",
        "from hparams import create_hparams\n",
        "from scipy.io.wavfile import write"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZXRcGbrzP_1"
      },
      "source": [
        "text_file = \"sample_texts/alice-in-wonderland.txt\"\n",
        "genlist = []\n",
        "with open(text_file) as file:\n",
        "  for line in file:\n",
        "    genlist.append(line.strip())\n",
        "\n",
        "print(genlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2f66m9EWdCE"
      },
      "source": [
        "# Vars to pass: TT2 path, waveglow path, text (.txt file with newlines, or an array), save location, toggle one/multi-file\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "import sys\n",
        "sys.path.append('waveglow/')\n",
        "import numpy as np\n",
        "import torch\n",
        "import epitran\n",
        "import eng_to_ipa as ipa\n",
        "import re\n",
        "\n",
        "from model import Tacotron2\n",
        "from layers import TacotronSTFT, STFT\n",
        "from audio_processing import griffin_lim\n",
        "from train import load_model\n",
        "from text import text_to_sequence\n",
        "from denoiser import Denoiser\n",
        "from text.cleaners import english_cleaners\n",
        "from utils import make_arpabet\n",
        "from hparams import create_hparams\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "text = [\"That quick beige fox jumped in the air over each thin dog, look out, I shout, for he's foiled you again, creating chaos.\",\n",
        "\"Are those shy Eurasian footwear, cowboy chaps, or jolly earthmoving headgear.\",]\n",
        "\n",
        "checkpoint_path = \"outdir/checkpoint_10000\"\n",
        "waveglow_path = 'waveglow/checkpoints/waveglow_50000'\n",
        "\n",
        "def synth_multi(checkpoint_path, waveglow_path, text, savedir):\n",
        "\n",
        "  if not os.path.exists(savedir):\n",
        "    os.makedirs(savedir)\n",
        "    print(\"Creating directory \" + savedir + \"...\")\n",
        "\n",
        "  hparams = create_hparams()\n",
        "  hparams.sampling_rate = 22050\n",
        "\n",
        "  checkpoint_path = \"outdir/checkpoint_8000\"\n",
        "  model = load_model(hparams)\n",
        "  model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n",
        "  _ = model.cuda().eval().half()\n",
        "\n",
        "  waveglow_path = 'waveglow/checkpoints/waveglow_50000'\n",
        "  waveglow = torch.load(waveglow_path)['model']\n",
        "  waveglow.cuda().eval().half()\n",
        "  for k in waveglow.convinv:\n",
        "      k.float()\n",
        "  denoiser = Denoiser(waveglow)\n",
        "\n",
        "  for entry in text:\n",
        "    wav_name = \"_\".join(entry.split(\" \")[:4]).lower() + \".wav\"\n",
        "\n",
        "    epi = epitran.Epitran('eng-Latn', ligatures = True)\n",
        "    if hparams.preprocessing == \"ipa\":\n",
        "      entry = ipa.convert(english_cleaners(entry))\n",
        "      foreign_words = re.findall(r\"[^ ]{0,}\\*\", entry)\n",
        "      for word in foreign_words:\n",
        "        entry = entry.replace(word, epi.transliterate(word[0:len(word)-1]))\n",
        "    if hparams.preprocessing == \"arpabet\":\n",
        "      entry = make_arpabet(entry)\n",
        "\n",
        "    # Text sequencer\n",
        "    if hparams.preprocessing is not None:\n",
        "      sequence = np.array(text_to_sequence(entry, None))[None, :]\n",
        "    else:\n",
        "      sequence = np.array(text_to_sequence(entry, ['english_cleaners']))[None, :]\n",
        "    sequence = torch.autograd.Variable(\n",
        "      torch.from_numpy(sequence)).cuda().long()\n",
        "\n",
        "    # Synthesis\n",
        "    mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
        "    with torch.no_grad():\n",
        "      audio = waveglow.infer(mel_outputs_postnet, sigma=0.666)\n",
        "    audio_denoised = denoiser(audio, strength=0.01)[:, 0]\n",
        "\n",
        "    # Save audio\n",
        "    print (\"Saving \" + wav_name)\n",
        "    write(os.path.join(savedir, wav_name), hparams.sampling_rate, audio_denoised[0].data.cpu().numpy())\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "\n",
        "  parser.add_argument('-t', '--tt2_checkpoint_path', type=str, default=\"tacotron2_statedict.pt\",\n",
        "                        required=False, help='Tacotron2 checkpoint to load')\n",
        "  parser.add_argument('-w', '--waveglow_checkpoint_path', type=str, default=\"waveglow/waveglow_256channels_universal_v5.pt\",\n",
        "                        required=False, help='waveglow checkpoint to load')\n",
        "  parser.add_argument('-f', '--text_file', type=str, \n",
        "                        help='Text file or list to generate audio from.')\n",
        "  parser.add_argument('-o', '--output_directory', type=str, default=\"savedir\",\n",
        "                        required=False, help='Output directory to save to. Defaults to savedir.')\n",
        "  \n",
        "  args = parser.parse_args()\n",
        "\n",
        "  synth_multi(args.tt2_checkpoint_path, args.waveglow_checkpoint_path, args.text_file, args.output_directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYP5jfsIttSG"
      },
      "source": [
        " cd \"/content/drive/My Drive/ML/tacotron2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRFxgkpMrCEx"
      },
      "source": [
        "!python generate_from_file.py -t \"outdir/checkpoint_20000\" -w \"waveglow/checkpoints/waveglow_130000\" -f \"sample_texts/all-phonemes.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH4PcD5j1y80"
      },
      "source": [
        "import IPython.display as ipd\n",
        "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyID9T8sOkth"
      },
      "source": [
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import IPython.display as ipd\n",
        "\n",
        "import sys\n",
        "sys.path.append('waveglow/')\n",
        "import numpy as np\n",
        "import torch\n",
        "import epitran\n",
        "import eng_to_ipa as ipa\n",
        "import re\n",
        "\n",
        "from hparams import create_hparams\n",
        "from model import Tacotron2\n",
        "from layers import TacotronSTFT, STFT\n",
        "from audio_processing import griffin_lim\n",
        "from train import load_model\n",
        "from text import text_to_sequence\n",
        "from denoiser import Denoiser\n",
        "from text.cleaners import english_cleaners\n",
        "\n",
        "\n",
        "\n",
        "text = \"Alice opened the door and found that it led into a small passage, not much larger than a rat-hole. \"\n",
        "text_debug = text\n",
        "\n",
        "#epitran preprocessing loop\n",
        "hparams = create_hparams()\n",
        "epi = epitran.Epitran('eng-Latn')\n",
        "if hparams.ipa_preprocessing:\n",
        "  text = ipa.convert(english_cleaners(text))\n",
        "  foreign_words = re.findall(r\"[^ ]{0,}\\*\", text)\n",
        "  for word in foreign_words:\n",
        "    debug_text = text.replace(word, epi.transliterate(word[0:len(word)-1]))\n",
        "\n",
        "def ord_debug_simple(entry):\n",
        "  chars = []\n",
        "  wordlist = []\n",
        "  list = entry.split()\n",
        "  for word in list:\n",
        "    chars = []\n",
        "    chars += word\n",
        "    chars = [ord(char) for char in chars]\n",
        "    wordlist.append(chars)\n",
        "      \n",
        "  return str(wordlist)\n",
        "\n",
        "print(\"english_cleaners: \\t\" + english_cleaners(text_debug))\n",
        "print(\"eng_to_ipa: \\t\\t\" + ipa.convert(english_cleaners(text_debug)))\n",
        "print(\"epitran: \\t\\t\" + epi.transliterate(english_cleaners(text_debug)))\n",
        "print(\"\\n\")\n",
        "print(\"hybrid_mix: \\t\\t\" + debug_text)\n",
        "print(\"hybrid_mix_cleaned: \\t\" + english_cleaners(debug_text))\n",
        "print(\"\\n\")\n",
        "print(\"english_cleaners_ORD: \\t\" + ord_debug_simple(text_debug))\n",
        "print(\"hybrid_mix_ORD: \\t\" + ord_debug_simple(debug_text))\n",
        "print(\"hybrid_cleanded_ORD: \\t\" + ord_debug_simple(english_cleaners(debug_text)))\n",
        "print(\"\\n\")\n",
        "print(\"eng_to_ipa_ORD: \\t\" + ord_debug_simple(ipa.convert(english_cleaners(text_debug))))\n",
        "print(\"epitran_ORD: \\t\\t\" + ord_debug_simple(epi.transliterate(english_cleaners(text_debug))))\n",
        "\n",
        "#text sequencer\n",
        "sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
        "sequence = torch.autograd.Variable(\n",
        "    torch.from_numpy(sequence)).cuda().long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mxgycMwlRFw"
      },
      "source": [
        "#Tester\n",
        "\n",
        "from hparams import create_hparams\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import re\n",
        "from text import cleaners\n",
        "from text.symbols import symbols\n",
        "\n",
        "import layers\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text, convert_to_ipa, convert_to_arpa\n",
        "from text import text_to_sequence\n",
        "from text import cleaners\n",
        "\n",
        "apath = '../dataset/train.txt'\n",
        "hparams = create_hparams()\n",
        "\n",
        "class TextMelLoader(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "        1) loads audio,text pairs\n",
        "        2) normalizes text and converts them to sequences of one-hot vectors\n",
        "        3) computes mel-spectrograms from audio files.\n",
        "    \"\"\"\n",
        "    def __init__(self, audiopaths_and_text, hparams):\n",
        "        self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)[:5]\n",
        "        self.text_cleaners = hparams.text_cleaners\n",
        "        print(self.audiopaths_and_text)\n",
        "        if hparams.preprocessing is not None:\n",
        "            if hparams.preprocessing == 'ipa':\n",
        "                convert_to_ipa(self.audiopaths_and_text)\n",
        "            if hparams.preprocessing == 'arpabet':\n",
        "                convert_to_arpa(self.audiopaths_and_text)\n",
        "            self.text_cleaners = None\n",
        "        print(self.audiopaths_and_text)\n",
        "        self.max_wav_value = hparams.max_wav_value\n",
        "        self.sampling_rate = hparams.sampling_rate\n",
        "        self.load_mel_from_disk = hparams.load_mel_from_disk\n",
        "        self.stft = layers.TacotronSTFT(\n",
        "            hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "            hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "            hparams.mel_fmax)\n",
        "        random.seed(hparams.seed)\n",
        "        random.shuffle(self.audiopaths_and_text)\n",
        "\n",
        "    def get_mel_text_pair(self, audiopath_and_text):\n",
        "        # separate filename and text\n",
        "        audiopath, text = audiopath_and_text[0], audiopath_and_text[1]\n",
        "        text = self.get_text(text)\n",
        "        mel = self.get_mel(audiopath)\n",
        "        return (text, mel)\n",
        "\n",
        "    def get_mel(self, filename):\n",
        "        if not self.load_mel_from_disk:\n",
        "            audio, sampling_rate = load_wav_to_torch(filename)\n",
        "            if sampling_rate != self.stft.sampling_rate:\n",
        "                raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "                    sampling_rate, self.stft.sampling_rate))\n",
        "            audio_norm = audio / self.max_wav_value\n",
        "            audio_norm = audio_norm.unsqueeze(0)\n",
        "            audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "            melspec = self.stft.mel_spectrogram(audio_norm)\n",
        "            melspec = torch.squeeze(melspec, 0)\n",
        "        else:\n",
        "            melspec = torch.from_numpy(np.load(filename))\n",
        "            assert melspec.size(0) == self.stft.n_mel_channels, (\n",
        "                'Mel dimension mismatch: given {}, expected {}'.format(\n",
        "                    melspec.size(0), self.stft.n_mel_channels))\n",
        "\n",
        "        return melspec\n",
        "\n",
        "    def get_text(self, text):\n",
        "        text_norm = torch.IntTensor(text_to_sequence(text, self.text_cleaners))\n",
        "        return text_norm\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_mel_text_pair(self.audiopaths_and_text[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audiopaths_and_text)\n",
        "\n",
        "\n",
        "def text_to_sequence(text, cleaner_names):\n",
        "  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
        "\n",
        "    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n",
        "    in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n",
        "\n",
        "    Args:\n",
        "      text: string to convert to a sequence\n",
        "      cleaner_names: names of the cleaner functions to run the text through\n",
        "\n",
        "    Returns:\n",
        "      List of integers corresponding to the symbols in the text\n",
        "  '''\n",
        "  sequence = []\n",
        "\n",
        "  # Check for curly braces and treat their contents as ARPAbet:\n",
        "  while len(text):\n",
        "    m = _curly_re.match(text)\n",
        "    print(m)\n",
        "    if not m:\n",
        "      sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n",
        "      break\n",
        "    sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n",
        "    sequence += _arpabet_to_sequence(m.group(2))\n",
        "    text = m.group(3)\n",
        "\n",
        "  return sequence\n",
        "\n",
        "def sequence_to_text(sequence):\n",
        "  '''Converts a sequence of IDs back to a string'''\n",
        "  result = ''\n",
        "  for symbol_id in sequence:\n",
        "    if symbol_id in _id_to_symbol:\n",
        "      s = _id_to_symbol[symbol_id]\n",
        "      # Enclose ARPAbet back in curly braces:\n",
        "      if len(s) > 1 and s[0] == '@':\n",
        "        s = '{%s}' % s[1:]\n",
        "      result += s\n",
        "  return result.replace('}{', ' ')\n",
        "\n",
        "\n",
        "def _clean_text(text, cleaner_names):\n",
        "  if cleaner_names is None:\n",
        "    return text\n",
        "  else:\n",
        "    for name in cleaner_names:\n",
        "      cleaner = getattr(cleaners, name)\n",
        "      if not cleaner:\n",
        "        raise Exception('Unknown cleaner: %s' % name)\n",
        "      text = cleaner(text)\n",
        "  return text\n",
        "\n",
        "\n",
        "def _symbols_to_sequence(symbols):\n",
        "  return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n",
        "\n",
        "\n",
        "def _arpabet_to_sequence(text):\n",
        "  return _symbols_to_sequence(['@' + s for s in text.split()])\n",
        "\n",
        "\n",
        "def _should_keep_symbol(s):\n",
        "  return s in _symbol_to_id and s is not '_' and s is not '~'\n",
        "\n",
        "\n",
        "text = \"{DH AH0} {B EY1 ZH} {HH Y UW1} {AA1 N} {DH AH0} {W AO1 T ER0 Z} {AH1 V} {DH AH0} {L AA1 K} {IH0 M P R EH1 S T} {AO1 L}, {IH0 N K L UW1 D IH0 NG} {DH AH0} {F R EH1 N CH} {K W IY1 N}, {B IH0 F AO1 R} {SH IY1} {HH ER1 D} {DH AE1 T} {S IH1 M F AH0 N IY0} {AH0 G EH1 N} {JH AH1 S T} {AE1 Z} {Y AH1 NG} {AA1 R TH ER0} {W AA1 N T AH0 D}.\"\n",
        "# Mappings from symbol to numeric ID and vice versa:\n",
        "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
        "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
        "\n",
        "# Regular expression matching text enclosed in curly braces:\n",
        "_curly_re = re.compile(r'(.*?)\\{(.+?)\\}(.*)')\n",
        "\n",
        "seq = text_to_sequence(text, None)\n",
        "print(seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF6rvm9etBRo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}